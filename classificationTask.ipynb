{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab5ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "with open(\"config.yml\", 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "path_fname = cfg['path_fname']\n",
    "fname = cfg['fname']\n",
    "output_dir = cfg['output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "468dd1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\immo\\.conda\\envs\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded FAISS index with 120 vectors from PreProcessing\\ProcessedFiles\\index\\faiss.index\n",
      "   • Loaded metadata for 120 documents from PreProcessing\\ProcessedFiles\\index\\docs.json\n"
     ]
    }
   ],
   "source": [
    "# Lod chunks from json file\n",
    "import json\n",
    "with open(\"Documents\\\\SORA_chunks_cleaned_manual.json\", 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "from PreProcessing.embeddingToolsFAISSv2 import EmbeddingToolFAISS\n",
    "embedder = EmbeddingToolFAISS( output_dir=Path(output_dir), index_backend=\"faiss\")\n",
    "embeddings = embedder.load_index()\n",
    "\n",
    "from RAG.ragv2 import RAG\n",
    "rag_system = RAG(embedding_tool=embedder, chunks=chunks, default_mode=\"hybrid\", reranker=\"colbert\")\n",
    "\n",
    "from LLM.LLM_openAI_Classification import LLMIndicatorAssistant, InitialOperationInput, IndicatorName\n",
    "engine = LLMIndicatorAssistant(rag_system=rag_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6f474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Dict\n",
    "\n",
    "# op = InitialOperationInput(\n",
    "#     maximum_takeoff_mass_category=\"lt_25kg\",  # lt_25kg or gte_25kg\n",
    "#     vlos_or_bvlos=\"VLOS\", # Visual Line of Sight VLOS or Beyond Visual Line of Sight BVLOS\n",
    "#     ground_environment=\"sparsely_populated\", # \"controlled_area\", \"sparsely_populated\", \"populated\"\n",
    "#     airspace_type=\"uncontrolled\", # \"controlled\", \"uncontrolled\"\n",
    "#     maximum_altitude_category=\"gt_50m_le_120m\",  # \"le_50m\", \"gt_50m_le_120m\", \"gt_120m_le_150m\", \"gt_150m\"\n",
    "# )\n",
    "\n",
    "# indicators = [\n",
    "#     \"likely_regulatory_pathway\",\n",
    "#     \"initial_ground_risk_orientation\",\n",
    "#     \"initial_air_risk_orientation\",\n",
    "#     \"expected_assessment_depth\",\n",
    "# ]\n",
    "\n",
    "# results: Dict[str, dict] = {}\n",
    "\n",
    "# for indicator in indicators:\n",
    "#     out = engine.answer_indicator(\n",
    "#         indicator=indicator,\n",
    "#         op=op,\n",
    "#         stream=False,\n",
    "#         print_sources=False,\n",
    "#     )\n",
    "#     results[indicator] = out[\"result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4dba978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Indicators:\")\n",
    "# for k, v in results.items():\n",
    "#     print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f07176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = \"initial_indicators.json\"\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(results, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b668649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency & Accuracy Pipeline loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Consistency & Accuracy Evaluation Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def text_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate similarity ratio between two texts (0 to 1).\"\"\"\n",
    "    if not text1 or not text2:\n",
    "        return 0.0 if (text1 or text2) else 1.0\n",
    "    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
    "\n",
    "def get_operation_key(op) -> str:\n",
    "    \"\"\"Generate a unique key for an operation input.\"\"\"\n",
    "    if isinstance(op, dict):\n",
    "        return f\"{op['maximum_takeoff_mass_category']}_{op['vlos_or_bvlos']}_{op['ground_environment']}_{op['airspace_type']}_{op['maximum_altitude_category']}\"\n",
    "    else:\n",
    "        return f\"{op.maximum_takeoff_mass_category}_{op.vlos_or_bvlos}_{op.ground_environment}_{op.airspace_type}_{op.maximum_altitude_category}\"\n",
    "\n",
    "def get_operation_file_path(save_path: Path, op) -> Path:\n",
    "    \"\"\"Get the file path for an operation result.\"\"\"\n",
    "    op_key = get_operation_key(op)\n",
    "    return save_path / f\"op_result_{op_key}.json\"\n",
    "\n",
    "def run_consistency_evaluation(\n",
    "    engine,\n",
    "    operation_inputs: List[InitialOperationInput],\n",
    "    indicators: List[str],\n",
    "    num_runs: int = 3,\n",
    "    verbose: bool = True,\n",
    "    save_path: str = None,\n",
    "    delay_between_calls: float = 1.0,\n",
    "    skip_existing: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Run the model multiple times for each input combination to check consistency.\n",
    "    \n",
    "    Args:\n",
    "        engine: The LLM engine\n",
    "        operation_inputs: List of operation inputs to test\n",
    "        indicators: List of indicator names to evaluate\n",
    "        num_runs: Number of runs per input combination\n",
    "        verbose: Whether to print progress\n",
    "        save_path: Path to save results after each operation (optional)\n",
    "        delay_between_calls: Delay in seconds between API calls to avoid rate limits\n",
    "        skip_existing: Whether to skip operations that already have saved results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with the last operation's results (all results saved to file).\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Setup save path\n",
    "    if save_path:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    for op_idx, op in enumerate(operation_inputs):\n",
    "        # Check if result file already exists\n",
    "        if save_path and skip_existing:\n",
    "            op_file = get_operation_file_path(save_path, op)\n",
    "            if op_file.exists():\n",
    "                if verbose:\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(f\"Operation Input {op_idx + 1}/{len(operation_inputs)} - SKIPPED (file exists)\")\n",
    "                    print(f\"  Mass: {op.maximum_takeoff_mass_category}, VLOS: {op.vlos_or_bvlos}\")\n",
    "                    print(f\"  Ground: {op.ground_environment}, Airspace: {op.airspace_type}\")\n",
    "                    print(f\"  Altitude: {op.maximum_altitude_category}\")\n",
    "                    print(f\"  File: {op_file}\")\n",
    "                    print(f\"{'='*60}\")\n",
    "                continue\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Operation Input {op_idx + 1}/{len(operation_inputs)}\")\n",
    "            print(f\"  Mass: {op.maximum_takeoff_mass_category}, VLOS: {op.vlos_or_bvlos}\")\n",
    "            print(f\"  Ground: {op.ground_environment}, Airspace: {op.airspace_type}\")\n",
    "            print(f\"  Altitude: {op.maximum_altitude_category}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        op_results = {\n",
    "            \"operation_input\": {\n",
    "                \"maximum_takeoff_mass_category\": op.maximum_takeoff_mass_category,\n",
    "                \"vlos_or_bvlos\": op.vlos_or_bvlos,\n",
    "                \"ground_environment\": op.ground_environment,\n",
    "                \"airspace_type\": op.airspace_type,\n",
    "                \"maximum_altitude_category\": op.maximum_altitude_category,\n",
    "            },\n",
    "            \"runs\": [],\n",
    "            \"consistency_metrics\": {}\n",
    "        }\n",
    "        \n",
    "        # Run multiple times\n",
    "        for run_idx in range(num_runs):\n",
    "            if verbose:\n",
    "                print(f\"  Run {run_idx + 1}/{num_runs}...\", end=\" \", flush=True)\n",
    "            \n",
    "            run_results = {}\n",
    "            for indicator in indicators:\n",
    "                out = engine.answer_indicator(\n",
    "                    indicator=indicator,\n",
    "                    op=op,\n",
    "                    stream=False,\n",
    "                    print_sources=False,\n",
    "                )\n",
    "                run_results[indicator] = {\n",
    "                    \"value\": out[\"result\"].get(\"value\"),\n",
    "                    \"explanation\": out[\"result\"].get(\"explanation\")\n",
    "                }\n",
    "                \n",
    "                # Add delay between API calls to avoid rate limiting\n",
    "                if delay_between_calls > 0:\n",
    "                    time.sleep(delay_between_calls)\n",
    "            \n",
    "            op_results[\"runs\"].append(run_results)\n",
    "            if verbose:\n",
    "                print(\"Done\")\n",
    "        \n",
    "        # Calculate consistency metrics for this operation input\n",
    "        for indicator in indicators:\n",
    "            values = [run[indicator][\"value\"] for run in op_results[\"runs\"]]\n",
    "            explanations = [run[indicator][\"explanation\"] for run in op_results[\"runs\"]]\n",
    "            \n",
    "            # Value consistency: percentage of runs with the most common value\n",
    "            value_counts = Counter(values)\n",
    "            most_common_value, most_common_count = value_counts.most_common(1)[0]\n",
    "            value_consistency = most_common_count / num_runs\n",
    "            \n",
    "            # Explanation consistency: average pairwise similarity\n",
    "            explanation_similarities = []\n",
    "            for i in range(len(explanations)):\n",
    "                for j in range(i + 1, len(explanations)):\n",
    "                    explanation_similarities.append(text_similarity(explanations[i], explanations[j]))\n",
    "            avg_explanation_similarity = np.mean(explanation_similarities) if explanation_similarities else 1.0\n",
    "            \n",
    "            op_results[\"consistency_metrics\"][indicator] = {\n",
    "                \"value_consistency\": value_consistency,\n",
    "                \"most_common_value\": most_common_value,\n",
    "                \"unique_values\": list(value_counts.keys()),\n",
    "                \"explanation_similarity\": avg_explanation_similarity,\n",
    "            }\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  {indicator}:\")\n",
    "                print(f\"    Value consistency: {value_consistency:.1%} (most common: '{most_common_value}')\")\n",
    "                print(f\"    Explanation similarity: {avg_explanation_similarity:.1%}\")\n",
    "        \n",
    "        # Save individual operation results after all runs are complete\n",
    "        if save_path:\n",
    "            op_file = get_operation_file_path(save_path, op)\n",
    "            \n",
    "            with open(op_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(op_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Saved operation results to {op_file}\")\n",
    "        \n",
    "        # Explicitly clean up run data to free memory\n",
    "        del op_results\n",
    "\n",
    "\n",
    "\n",
    "def load_operation_results(save_dir: str, verbose: bool = False) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load all operation results from saved files in the directory.\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory containing saved operation result files\n",
    "        verbose: Whether to print information about loaded files\n",
    "    \n",
    "    Returns:\n",
    "        List of operation result dictionaries\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    results = []\n",
    "    \n",
    "    # Find all op_result_*.json files in the directory\n",
    "    op_files = sorted(save_dir.glob(\"op_result_*.json\"))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Loading results from {save_dir}\")\n",
    "        print(f\"Found {len(op_files)} operation result files\")\n",
    "    \n",
    "    for op_file in op_files:\n",
    "        with open(op_file, 'r', encoding='utf-8') as f:\n",
    "            op_result = json.load(f)\n",
    "            results.append(op_result)\n",
    "            if verbose:\n",
    "                op_key = get_operation_key(op_result[\"operation_input\"])\n",
    "                num_runs = len(op_result.get(\"runs\", []))\n",
    "                print(f\"  Loaded: {op_file.name} ({num_runs} runs)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_accuracy_vs_ground_truth(\n",
    "    save_dir: str,\n",
    "    ground_truth: dict,\n",
    "    indicators: List[str],\n",
    "    verbose: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compare model outputs with ground truth and calculate accuracy.\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory containing saved operation result files\n",
    "        ground_truth: Dict with ground truth values per operation input key\n",
    "        indicators: List of indicator names\n",
    "        verbose: Whether to print progress and details\n",
    "    \n",
    "    Returns:\n",
    "        Accuracy metrics dictionary\n",
    "    \"\"\"\n",
    "    # Load all saved results from files\n",
    "    results = load_operation_results(save_dir, verbose=verbose)\n",
    "    \n",
    "    if not results:\n",
    "        print(f\"Warning: No operation results found in {save_dir}\")\n",
    "        return {\n",
    "            \"per_indicator\": {ind: {\"correct\": 0, \"total\": 0, \"accuracy\": 0.0} for ind in indicators},\n",
    "            \"per_operation\": [],\n",
    "            \"overall\": {\"correct\": 0, \"total\": 0, \"accuracy\": 0.0}\n",
    "        }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nCalculating accuracy for {len(results)} operations against ground truth...\")\n",
    "    \n",
    "    accuracy_metrics = {\n",
    "        \"per_indicator\": {ind: {\"correct\": 0, \"total\": 0} for ind in indicators},\n",
    "        \"per_operation\": [],\n",
    "        \"overall\": {\"correct\": 0, \"total\": 0}\n",
    "    }\n",
    "    \n",
    "    for op_result in results:\n",
    "        op_input = op_result[\"operation_input\"]\n",
    "        \n",
    "        # Create a key to match with ground truth\n",
    "        op_key = get_operation_key(op_input)\n",
    "        \n",
    "        if op_key not in ground_truth:\n",
    "            if verbose:\n",
    "                print(f\"Warning: No ground truth found for operation key: {op_key}\")\n",
    "            continue\n",
    "        \n",
    "        gt = ground_truth[op_key]\n",
    "        op_accuracy = {\"operation_key\": op_key, \"indicators\": {}}\n",
    "        \n",
    "        for indicator in indicators:\n",
    "            # Use the most common value from the runs\n",
    "            predicted_value = op_result[\"consistency_metrics\"][indicator][\"most_common_value\"]\n",
    "            gt_value = gt.get(indicator, {}).get(\"value\")\n",
    "            \n",
    "            if gt_value is None:\n",
    "                if verbose:\n",
    "                    print(f\"  Warning: No ground truth value for indicator '{indicator}' in operation '{op_key}'\")\n",
    "                continue\n",
    "            \n",
    "            is_correct = predicted_value.lower().strip() == gt_value.lower().strip()\n",
    "            \n",
    "            accuracy_metrics[\"per_indicator\"][indicator][\"total\"] += 1\n",
    "            accuracy_metrics[\"overall\"][\"total\"] += 1\n",
    "            \n",
    "            if is_correct:\n",
    "                accuracy_metrics[\"per_indicator\"][indicator][\"correct\"] += 1\n",
    "                accuracy_metrics[\"overall\"][\"correct\"] += 1\n",
    "            \n",
    "            op_accuracy[\"indicators\"][indicator] = {\n",
    "                \"predicted\": predicted_value,\n",
    "                \"ground_truth\": gt_value,\n",
    "                \"correct\": is_correct\n",
    "            }\n",
    "        \n",
    "        accuracy_metrics[\"per_operation\"].append(op_accuracy)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    for indicator in indicators:\n",
    "        stats = accuracy_metrics[\"per_indicator\"][indicator]\n",
    "        stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"] if stats[\"total\"] > 0 else 0.0\n",
    "    \n",
    "    overall = accuracy_metrics[\"overall\"]\n",
    "    overall[\"accuracy\"] = overall[\"correct\"] / overall[\"total\"] if overall[\"total\"] > 0 else 0.0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Accuracy calculation complete. Evaluated {len(accuracy_metrics['per_operation'])} operations.\")\n",
    "    \n",
    "    return accuracy_metrics\n",
    "\n",
    "\n",
    "def print_summary(save_dir: str, accuracy_metrics: dict, indicators: List[str], verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Print a summary of all metrics.\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory containing saved operation result files\n",
    "        accuracy_metrics: Accuracy metrics dictionary from calculate_accuracy_vs_ground_truth\n",
    "        indicators: List of indicator names\n",
    "        verbose: Whether to print detailed loading information\n",
    "    \n",
    "    Returns:\n",
    "        Summary dictionary with consistency and accuracy metrics\n",
    "    \"\"\"\n",
    "    # Load all saved results from files\n",
    "    consistency_results = load_operation_results(save_dir, verbose=False)\n",
    "    \n",
    "    if not consistency_results:\n",
    "        print(f\"Warning: No operation results found in {save_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nLoaded {len(consistency_results)} operation results from {save_dir}\")\n",
    "    \n",
    "    # Consistency Summary\n",
    "    print(\"\\n--- CONSISTENCY METRICS ---\")\n",
    "    avg_value_consistency = {}\n",
    "    avg_explanation_similarity = {}\n",
    "    \n",
    "    for indicator in indicators:\n",
    "        value_consistencies = []\n",
    "        explanation_sims = []\n",
    "        \n",
    "        for r in consistency_results:\n",
    "            if indicator in r.get(\"consistency_metrics\", {}):\n",
    "                value_consistencies.append(r[\"consistency_metrics\"][indicator][\"value_consistency\"])\n",
    "                explanation_sims.append(r[\"consistency_metrics\"][indicator][\"explanation_similarity\"])\n",
    "        \n",
    "        if value_consistencies:\n",
    "            avg_value_consistency[indicator] = np.mean(value_consistencies)\n",
    "            avg_explanation_similarity[indicator] = np.mean(explanation_sims)\n",
    "            \n",
    "            print(f\"\\n{indicator}:\")\n",
    "            print(f\"  Avg Value Consistency:      {avg_value_consistency[indicator]:.1%}\")\n",
    "            print(f\"  Avg Explanation Similarity: {avg_explanation_similarity[indicator]:.1%}\")\n",
    "        else:\n",
    "            print(f\"\\n{indicator}: No data available\")\n",
    "            avg_value_consistency[indicator] = 0.0\n",
    "            avg_explanation_similarity[indicator] = 0.0\n",
    "    \n",
    "    overall_value_consistency = np.mean(list(avg_value_consistency.values())) if avg_value_consistency else 0.0\n",
    "    overall_explanation_similarity = np.mean(list(avg_explanation_similarity.values())) if avg_explanation_similarity else 0.0\n",
    "    \n",
    "    print(f\"\\nOVERALL CONSISTENCY:\")\n",
    "    print(f\"  Value Consistency:      {overall_value_consistency:.1%}\")\n",
    "    print(f\"  Explanation Similarity: {overall_explanation_similarity:.1%}\")\n",
    "    \n",
    "    # Accuracy Summary\n",
    "    print(\"\\n--- ACCURACY VS GROUND TRUTH ---\")\n",
    "    for indicator in indicators:\n",
    "        stats = accuracy_metrics[\"per_indicator\"].get(indicator, {\"accuracy\": 0, \"correct\": 0, \"total\": 0})\n",
    "        print(f\"\\n{indicator}:\")\n",
    "        print(f\"  Accuracy: {stats['accuracy']:.1%} ({stats['correct']}/{stats['total']})\")\n",
    "    \n",
    "    overall = accuracy_metrics.get(\"overall\", {\"accuracy\": 0, \"correct\": 0, \"total\": 0})\n",
    "    print(f\"\\nOVERALL ACCURACY: {overall['accuracy']:.1%} ({overall['correct']}/{overall['total']})\")\n",
    "    \n",
    "    return {\n",
    "        \"num_operations\": len(consistency_results),\n",
    "        \"consistency\": {\n",
    "            \"avg_value_consistency\": overall_value_consistency,\n",
    "            \"avg_explanation_similarity\": overall_explanation_similarity,\n",
    "            \"per_indicator_value_consistency\": avg_value_consistency,\n",
    "            \"per_indicator_explanation_similarity\": avg_explanation_similarity,\n",
    "        },\n",
    "        \"accuracy\": {\n",
    "            \"overall\": overall[\"accuracy\"],\n",
    "            \"per_indicator\": {ind: accuracy_metrics[\"per_indicator\"][ind][\"accuracy\"] for ind in indicators if ind in accuracy_metrics[\"per_indicator\"]}\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Consistency & Accuracy Pipeline loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874e969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 3 operation inputs with 4 indicators each\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Define test operation inputs (different combinations)\n",
    "# =============================================================================\n",
    "\n",
    "test_operations = [\n",
    "    InitialOperationInput(\n",
    "        maximum_takeoff_mass_category=\"lt_25kg\",\n",
    "        vlos_or_bvlos=\"VLOS\",\n",
    "        ground_environment=\"sparsely_populated\",\n",
    "        airspace_type=\"uncontrolled\",\n",
    "        maximum_altitude_category=\"gt_50m_le_120m\",\n",
    "    ),\n",
    "    InitialOperationInput(\n",
    "        maximum_takeoff_mass_category=\"lt_25kg\",\n",
    "        vlos_or_bvlos=\"BVLOS\",\n",
    "        ground_environment=\"populated\",\n",
    "        airspace_type=\"controlled\",\n",
    "        maximum_altitude_category=\"gt_120m_le_150m\",\n",
    "    ),\n",
    "    InitialOperationInput(\n",
    "        maximum_takeoff_mass_category=\"gte_25kg\",\n",
    "        vlos_or_bvlos=\"BVLOS\",\n",
    "        ground_environment=\"controlled_area\",\n",
    "        airspace_type=\"uncontrolled\",\n",
    "        maximum_altitude_category=\"le_50m\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# List of indicators to evaluate\n",
    "eval_indicators = [\n",
    "    \"likely_regulatory_pathway\",\n",
    "    \"initial_ground_risk_orientation\",\n",
    "    \"initial_air_risk_orientation\",\n",
    "    \"expected_assessment_depth\",\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(test_operations)} operation inputs with {len(eval_indicators)} indicators each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f365a6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Operation Input 1/3 - SKIPPED (file exists)\n",
      "  Mass: lt_25kg, VLOS: VLOS\n",
      "  Ground: sparsely_populated, Airspace: uncontrolled\n",
      "  Altitude: gt_50m_le_120m\n",
      "  File: Results\\ClassificationTask\\op_result_lt_25kg_VLOS_sparsely_populated_uncontrolled_gt_50m_le_120m.json\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Operation Input 2/3 - SKIPPED (file exists)\n",
      "  Mass: lt_25kg, VLOS: BVLOS\n",
      "  Ground: populated, Airspace: controlled\n",
      "  Altitude: gt_120m_le_150m\n",
      "  File: Results\\ClassificationTask\\op_result_lt_25kg_BVLOS_populated_controlled_gt_120m_le_150m.json\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Operation Input 3/3 - SKIPPED (file exists)\n",
      "  Mass: gte_25kg, VLOS: BVLOS\n",
      "  Ground: controlled_area, Airspace: uncontrolled\n",
      "  Altitude: le_50m\n",
      "  File: Results\\ClassificationTask\\op_result_gte_25kg_BVLOS_controlled_area_uncontrolled_le_50m.json\n",
      "============================================================\n",
      "\n",
      "Consistency evaluation complete!\n",
      "Individual operation results saved to Results\\ClassificationTask\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Run Consistency Evaluation (multiple runs per input)\n",
    "# =============================================================================\n",
    "\n",
    "NUM_RUNS = 10  # Number of times to run each input combination\n",
    "SAVE_DIR = \"Results\\\\ClassificationTask\"\n",
    "DELAY_BETWEEN_CALLS = 1.0  # Seconds between API calls to avoid rate limits\n",
    "SKIP_EXISTING = True  # Skip operations that already have saved results\n",
    "\n",
    "run_consistency_evaluation(\n",
    "    engine=engine,\n",
    "    operation_inputs=test_operations,\n",
    "    indicators=eval_indicators,\n",
    "    num_runs=NUM_RUNS,\n",
    "    verbose=True,\n",
    "    save_path=SAVE_DIR,\n",
    "    delay_between_calls=DELAY_BETWEEN_CALLS,\n",
    "    skip_existing=SKIP_EXISTING\n",
    ")\n",
    "\n",
    "print(f\"\\nConsistency evaluation complete!\")\n",
    "print(f\"Individual operation results saved to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "658cc448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ground truth with 3 operation inputs\n",
      "Loading results from Results\\ClassificationTask\n",
      "Found 3 operation result files\n",
      "  Loaded: op_result_gte_25kg_BVLOS_controlled_area_uncontrolled_le_50m.json (10 runs)\n",
      "  Loaded: op_result_lt_25kg_BVLOS_populated_controlled_gt_120m_le_150m.json (10 runs)\n",
      "  Loaded: op_result_lt_25kg_VLOS_sparsely_populated_uncontrolled_gt_50m_le_120m.json (10 runs)\n",
      "\n",
      "Calculating accuracy for 3 operations against ground truth...\n",
      "Accuracy calculation complete. Evaluated 3 operations.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Ground Truth and Calculate Accuracy\n",
    "# =============================================================================\n",
    "\n",
    "# Load ground truth from file\n",
    "\n",
    "with open(\"Results\\\\ClassificationTask\\\\gt.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "print(f\"Loaded ground truth with {len(ground_truth)} operation inputs\")\n",
    "\n",
    "# Calculate accuracy using saved results\n",
    "accuracy_metrics = calculate_accuracy_vs_ground_truth(\n",
    "    save_dir=SAVE_DIR,\n",
    "    ground_truth=ground_truth,\n",
    "    indicators=eval_indicators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0a3f42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Loaded 3 operation results from Results\\ClassificationTask\n",
      "\n",
      "--- CONSISTENCY METRICS ---\n",
      "\n",
      "likely_regulatory_pathway:\n",
      "  Avg Value Consistency:      83.3%\n",
      "  Avg Explanation Similarity: 59.3%\n",
      "\n",
      "initial_ground_risk_orientation:\n",
      "  Avg Value Consistency:      86.7%\n",
      "  Avg Explanation Similarity: 43.9%\n",
      "\n",
      "initial_air_risk_orientation:\n",
      "  Avg Value Consistency:      96.7%\n",
      "  Avg Explanation Similarity: 60.2%\n",
      "\n",
      "expected_assessment_depth:\n",
      "  Avg Value Consistency:      100.0%\n",
      "  Avg Explanation Similarity: 66.9%\n",
      "\n",
      "OVERALL CONSISTENCY:\n",
      "  Value Consistency:      91.7%\n",
      "  Explanation Similarity: 57.6%\n",
      "\n",
      "--- ACCURACY VS GROUND TRUTH ---\n",
      "\n",
      "likely_regulatory_pathway:\n",
      "  Accuracy: 0.0% (0/3)\n",
      "\n",
      "initial_ground_risk_orientation:\n",
      "  Accuracy: 33.3% (1/3)\n",
      "\n",
      "initial_air_risk_orientation:\n",
      "  Accuracy: 33.3% (1/3)\n",
      "\n",
      "expected_assessment_depth:\n",
      "  Accuracy: 100.0% (3/3)\n",
      "\n",
      "OVERALL ACCURACY: 41.7% (5/12)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Print Summary of All Metrics\n",
    "# =============================================================================\n",
    "\n",
    "summary = print_summary(SAVE_DIR, accuracy_metrics, eval_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fec1137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed results saved to Results\\ClassificationTask\\evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Save Detailed Results\n",
    "# =============================================================================\n",
    "\n",
    "# Load consistency results from saved files for final output\n",
    "consistency_results_all = load_operation_results(SAVE_DIR)\n",
    "\n",
    "# Save all results to file for further analysis\n",
    "evaluation_output = {\n",
    "    \"num_runs\": NUM_RUNS,\n",
    "    \"num_operations\": len(test_operations),\n",
    "    \"indicators\": eval_indicators,\n",
    "    \"consistency_results\": consistency_results_all,\n",
    "    \"accuracy_metrics\": accuracy_metrics,\n",
    "    \"summary\": summary\n",
    "}\n",
    "\n",
    "output_path = \"Results\\\\ClassificationTask\\\\evaluation_results.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_output, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"Detailed results saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
